# Тестовое задание 

### Тестовое задание 
Используя фреймворк Scrapy необходимо написать парсер для получения информации о товарах интернет-магазина из списка категорий по заранее заданному шаблону, информация должна быть структурирована в виде словарей согласно формату выходных данных и сохранена в файл формата json.
На вход подается список категорий в виде ссылок (минимум 3), с количеством товаров от 100 штук на категорию
на сайте alkoteka.com (Например: https://alkoteka.com/catalog/slaboalkogolnye-napitki-2)
Обязательно осуществлять сбор данных с учетом выбранного региона для парсинга - Краснодар
По возможности для получения информации добавить возможность использовать подключение через прокси
### Входные данные
Изменяемый список ссылок на категории, можно выполнить в одном из двух вариантов:
В виде константной переменной формата: START_URLS = ["url_1", "url_2", ..., "url_3"]
В виде отдельного файла, который будет содержать список ссылок в произвольном формате

*** Обратите внимание, что тестовое задание выполняется только на Scrapy (без использования  Playwright, Headless, Selenium и тд)


# Реализация

Просмотрев все запросы, которые делает сайт https://alkoteka.com я нашла запросы к их API.
<br>Изменения в API происходят значительно реже, чем изменения в html, поэтому я выбираю сканировать API, а не html.

### Логика кравлера:
- получаем список городов по ссылке https://alkoteka.com/web-api/v1/city
- перебираем города и категории и формируем ссылки на продукты, вида https://alkoteka.com/web-api/v1/product?city_uuid=396df2b5-7b2b-11eb-80cd-00155d039009&options%5Bcategories%5D[]=sidr-medovukha&page=1&per_page=20&root_category_slug=slaboalkogolnye-napitki-2
- в каждый SKU добавляем поля с городом, чтобы сохранить знание о принадлежности записи к городу
- сохраняем все данные со всех страниц и разделов в единый массив в файл output.json

Если бы это была боевая задача, а не тестовая, я бы сохраняла данные в БД, выделив поля c количеством остатков по городам и ценой, чтобы не хранить множество раз описание одного и того же SKU.
Список категорий, которые нужно сканировать, я бы также поместила в таблицу в БД.

### Запуск кравлера:
```
scrapy crawl scan_alkoteka -a city_name="Ленинградская ст-ца"
```
